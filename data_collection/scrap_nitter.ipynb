{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Scraping Notebook Overview\n",
    "Welcome to the Twitter Scraping Notebook. This tool is designed for extracting tweets from specific Twitter accounts using Selenium for web scraping and BeautifulSoup for HTML parsing. Before starting, make sure you have installed the necessary libraries (`selenium`, `beautifulsoup4`, `pandas`) and have a WebDriver set up for Selenium. The code is organized into modular functions for ease of use and understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Setting Constants\n",
    "This section imports necessary Python libraries for web scraping, handling dates, and data manipulation. It also sets up important constants that will be used throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "END_DATE = datetime(2023, 10, 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Process Individual Tweets\n",
    "The `process_tweet` function is designed to extract key information from each tweet. It gathers data such as the tweet's ID, content, stats, and timestamp. The function is built to handle exceptions and returns a structured dictionary of tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(div, leaning):\n",
    "    try:\n",
    "        # Extracting tweet date\n",
    "        date_html = div.find_element(By.CLASS_NAME, 'tweet-date').get_attribute('innerHTML')\n",
    "        date_soup = BeautifulSoup(date_html, 'html.parser')\n",
    "        date_str = date_soup.a['title']\n",
    "        tweet_date = datetime.strptime(date_str, \"%b %d, %Y Â· %I:%M %p UTC\")\n",
    "\n",
    "        # Check for pinned, retweet, and quote tweets\n",
    "        if div.find_elements(By.CLASS_NAME, 'pinned') or \\\n",
    "           div.find_elements(By.CLASS_NAME, 'retweet-header') or \\\n",
    "           div.find_elements(By.CLASS_NAME, 'quote'):\n",
    "            return None\n",
    "\n",
    "        # Extracting tweet stats\n",
    "        tweet_stats = div.find_element(By.CLASS_NAME, 'tweet-stats').get_attribute('innerHTML')\n",
    "        soup = BeautifulSoup(tweet_stats, 'html.parser')\n",
    "        numbers = [stat.get_text(strip=True).replace(\",\", \"\") for stat in soup.find_all(class_=\"tweet-stat\")]\n",
    "\n",
    "        # Extracting tweet ID\n",
    "        tweet_id = div.find_element(By.CLASS_NAME, 'tweet-link').get_attribute('href')\n",
    "        parsed_url = urlparse(tweet_id)\n",
    "        path = parsed_url.path\n",
    "        id = path.split('/')[-1]\n",
    "\n",
    "        return {\n",
    "            'tweet_id': id,\n",
    "            'tweets': div.find_element(By.CLASS_NAME, 'tweet-content').get_attribute('innerHTML'),\n",
    "            'dates': date_str,\n",
    "            'username': div.find_element(By.CLASS_NAME, 'username').get_attribute('innerHTML'),\n",
    "            'url': tweet_id,\n",
    "            'replies': numbers[0],\n",
    "            'retweets': numbers[1],\n",
    "            'quote_retweets': numbers[2],\n",
    "            'likes': numbers[3],\n",
    "            'leaning': leaning\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing tweet: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Scraping Tweets from a Page\n",
    "`get_tweets_from_page` navigates to a Twitter page and scrapes tweets from it. It uses the `process_tweet` function for each tweet found and returns a list of tweet data in dictionary format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_from_page(driver, url, leaning):\n",
    "    tweet_data = []\n",
    "    driver.get(url)\n",
    "    elements = driver.find_elements(By.CLASS_NAME, 'timeline-item')\n",
    "\n",
    "    for div in elements:\n",
    "        data = process_tweet(div, leaning)\n",
    "        if data and data['dates'] >= END_DATE:\n",
    "            tweet_data.append(data)\n",
    "        elif data:\n",
    "            break\n",
    "\n",
    "    return tweet_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Save Data as CSV\n",
    "The `save_to_csv` function converts the list of tweet dictionaries into a pandas DataFrame and then saves this data into a CSV file. This function is crucial for data storage and later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Scraping Function by Username\n",
    "The `get_tweets_by_username` function is the main script for scraping tweets for a given Twitter username. It handles the browser setup with Selenium, manages page navigation, and calls the `save_to_csv` function to store the scraped data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_by_username(username, leaning):\n",
    "    driver = webdriver.Chrome()\n",
    "    url = f'https://nitter.rawbit.ninja/{username}'\n",
    "    all_tweets = []\n",
    "\n",
    "    while True:\n",
    "        tweets = get_tweets_from_page(driver, url, leaning)\n",
    "        if not tweets:\n",
    "            break\n",
    "        all_tweets.extend(tweets)\n",
    "\n",
    "        # Attempt to find and click \"Show more\" button\n",
    "        try:\n",
    "            button = driver.find_element(By.CLASS_NAME, \"show-more\")\n",
    "            button.click()\n",
    "            url = driver.current_url\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    driver.close()\n",
    "    save_to_csv(all_tweets, f'{username}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Scraping for Each User in the DataFrame\n",
    "This final cell runs the scraping process for each user listed in the 'clusters.csv' file. It iterates through each row in the DataFrame, scraping and saving tweets for each specified username. Make sure the CSV file is correctly formatted and available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clusters.csv')\n",
    "for index, row in df.iterrows():\n",
    "    get_tweets_by_username(row[0], row[1])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
